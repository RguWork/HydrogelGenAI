{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>avg_weight0</th>\n",
       "      <th>avg_weight1</th>\n",
       "      <th>avg_weight2</th>\n",
       "      <th>avg_water_loss1</th>\n",
       "      <th>avg_water_loss2</th>\n",
       "      <th>avg_total_water_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.15330</td>\n",
       "      <td>2.10955</td>\n",
       "      <td>2.09665</td>\n",
       "      <td>0.04375</td>\n",
       "      <td>0.01290</td>\n",
       "      <td>0.05665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.12135</td>\n",
       "      <td>2.08655</td>\n",
       "      <td>2.07245</td>\n",
       "      <td>0.03480</td>\n",
       "      <td>0.01410</td>\n",
       "      <td>0.04890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.91580</td>\n",
       "      <td>1.86810</td>\n",
       "      <td>1.86085</td>\n",
       "      <td>0.04770</td>\n",
       "      <td>0.00725</td>\n",
       "      <td>0.05495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.82995</td>\n",
       "      <td>1.77550</td>\n",
       "      <td>1.76390</td>\n",
       "      <td>0.05445</td>\n",
       "      <td>0.01160</td>\n",
       "      <td>0.06605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.44275</td>\n",
       "      <td>2.37980</td>\n",
       "      <td>2.37050</td>\n",
       "      <td>0.06295</td>\n",
       "      <td>0.00930</td>\n",
       "      <td>0.07225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2.21200</td>\n",
       "      <td>2.16675</td>\n",
       "      <td>2.15565</td>\n",
       "      <td>0.04525</td>\n",
       "      <td>0.01110</td>\n",
       "      <td>0.05635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2.43565</td>\n",
       "      <td>2.39155</td>\n",
       "      <td>2.38010</td>\n",
       "      <td>0.04410</td>\n",
       "      <td>0.01145</td>\n",
       "      <td>0.05555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2.48585</td>\n",
       "      <td>2.44075</td>\n",
       "      <td>2.42585</td>\n",
       "      <td>0.04510</td>\n",
       "      <td>0.01490</td>\n",
       "      <td>0.06000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3.27330</td>\n",
       "      <td>3.21245</td>\n",
       "      <td>3.19370</td>\n",
       "      <td>0.06085</td>\n",
       "      <td>0.01875</td>\n",
       "      <td>0.07960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3.24150</td>\n",
       "      <td>3.19515</td>\n",
       "      <td>3.17785</td>\n",
       "      <td>0.04635</td>\n",
       "      <td>0.01730</td>\n",
       "      <td>0.06365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2.01935</td>\n",
       "      <td>1.98205</td>\n",
       "      <td>1.97005</td>\n",
       "      <td>0.03730</td>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.04930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2.27030</td>\n",
       "      <td>2.21955</td>\n",
       "      <td>2.20590</td>\n",
       "      <td>0.05075</td>\n",
       "      <td>0.01365</td>\n",
       "      <td>0.06440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2.31520</td>\n",
       "      <td>2.27295</td>\n",
       "      <td>2.26300</td>\n",
       "      <td>0.04225</td>\n",
       "      <td>0.00995</td>\n",
       "      <td>0.05220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2.20380</td>\n",
       "      <td>2.16010</td>\n",
       "      <td>2.15110</td>\n",
       "      <td>0.04370</td>\n",
       "      <td>0.00900</td>\n",
       "      <td>0.05270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2.20545</td>\n",
       "      <td>2.16840</td>\n",
       "      <td>2.15680</td>\n",
       "      <td>0.03705</td>\n",
       "      <td>0.01160</td>\n",
       "      <td>0.04865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2.37940</td>\n",
       "      <td>2.34090</td>\n",
       "      <td>2.33110</td>\n",
       "      <td>0.03850</td>\n",
       "      <td>0.00980</td>\n",
       "      <td>0.04830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2.09155</td>\n",
       "      <td>2.05500</td>\n",
       "      <td>2.04540</td>\n",
       "      <td>0.03655</td>\n",
       "      <td>0.00960</td>\n",
       "      <td>0.04615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>2.21270</td>\n",
       "      <td>2.17320</td>\n",
       "      <td>2.16025</td>\n",
       "      <td>0.03950</td>\n",
       "      <td>0.01295</td>\n",
       "      <td>0.05245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>2.47340</td>\n",
       "      <td>2.41635</td>\n",
       "      <td>2.39850</td>\n",
       "      <td>0.05705</td>\n",
       "      <td>0.01785</td>\n",
       "      <td>0.07490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>2.19585</td>\n",
       "      <td>2.15805</td>\n",
       "      <td>2.14555</td>\n",
       "      <td>0.03780</td>\n",
       "      <td>0.01250</td>\n",
       "      <td>0.05030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>2.05615</td>\n",
       "      <td>2.01970</td>\n",
       "      <td>2.00675</td>\n",
       "      <td>0.03645</td>\n",
       "      <td>0.01295</td>\n",
       "      <td>0.04940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>2.29355</td>\n",
       "      <td>2.23680</td>\n",
       "      <td>2.22325</td>\n",
       "      <td>0.05675</td>\n",
       "      <td>0.01355</td>\n",
       "      <td>0.07030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>2.33575</td>\n",
       "      <td>2.29475</td>\n",
       "      <td>2.27560</td>\n",
       "      <td>0.04100</td>\n",
       "      <td>0.01915</td>\n",
       "      <td>0.06015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>2.30820</td>\n",
       "      <td>2.26680</td>\n",
       "      <td>2.25005</td>\n",
       "      <td>0.04140</td>\n",
       "      <td>0.01675</td>\n",
       "      <td>0.05815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>2.53975</td>\n",
       "      <td>2.49390</td>\n",
       "      <td>2.47980</td>\n",
       "      <td>0.04585</td>\n",
       "      <td>0.01410</td>\n",
       "      <td>0.05995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>2.06395</td>\n",
       "      <td>2.02670</td>\n",
       "      <td>2.01365</td>\n",
       "      <td>0.03725</td>\n",
       "      <td>0.01305</td>\n",
       "      <td>0.05030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>2.26795</td>\n",
       "      <td>2.22315</td>\n",
       "      <td>2.21160</td>\n",
       "      <td>0.04480</td>\n",
       "      <td>0.01155</td>\n",
       "      <td>0.05635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1.96460</td>\n",
       "      <td>1.91845</td>\n",
       "      <td>1.90915</td>\n",
       "      <td>0.04615</td>\n",
       "      <td>0.00930</td>\n",
       "      <td>0.05545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>2.38225</td>\n",
       "      <td>2.33015</td>\n",
       "      <td>2.32405</td>\n",
       "      <td>0.05210</td>\n",
       "      <td>0.00610</td>\n",
       "      <td>0.05820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>2.34140</td>\n",
       "      <td>2.29150</td>\n",
       "      <td>2.27720</td>\n",
       "      <td>0.04990</td>\n",
       "      <td>0.01430</td>\n",
       "      <td>0.06420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>2.65305</td>\n",
       "      <td>2.59760</td>\n",
       "      <td>2.58695</td>\n",
       "      <td>0.05545</td>\n",
       "      <td>0.01065</td>\n",
       "      <td>0.06610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>2.10255</td>\n",
       "      <td>2.05030</td>\n",
       "      <td>2.04230</td>\n",
       "      <td>0.05225</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.06025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>2.09910</td>\n",
       "      <td>2.06165</td>\n",
       "      <td>2.05065</td>\n",
       "      <td>0.03745</td>\n",
       "      <td>0.01100</td>\n",
       "      <td>0.04845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>2.40690</td>\n",
       "      <td>2.36495</td>\n",
       "      <td>2.34725</td>\n",
       "      <td>0.04195</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>0.05965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>1.98990</td>\n",
       "      <td>1.95965</td>\n",
       "      <td>1.94915</td>\n",
       "      <td>0.03025</td>\n",
       "      <td>0.01050</td>\n",
       "      <td>0.04075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>2.24795</td>\n",
       "      <td>2.20805</td>\n",
       "      <td>2.19890</td>\n",
       "      <td>0.03990</td>\n",
       "      <td>0.00915</td>\n",
       "      <td>0.04905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>2.01670</td>\n",
       "      <td>1.98560</td>\n",
       "      <td>1.97275</td>\n",
       "      <td>0.03110</td>\n",
       "      <td>0.01285</td>\n",
       "      <td>0.04395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>2.27710</td>\n",
       "      <td>2.24255</td>\n",
       "      <td>2.23000</td>\n",
       "      <td>0.03455</td>\n",
       "      <td>0.01255</td>\n",
       "      <td>0.04710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>2.41835</td>\n",
       "      <td>2.37710</td>\n",
       "      <td>2.36575</td>\n",
       "      <td>0.04125</td>\n",
       "      <td>0.01135</td>\n",
       "      <td>0.05260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>2.11270</td>\n",
       "      <td>2.07560</td>\n",
       "      <td>2.06605</td>\n",
       "      <td>0.03710</td>\n",
       "      <td>0.00955</td>\n",
       "      <td>0.04665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>2.20500</td>\n",
       "      <td>2.16250</td>\n",
       "      <td>2.15270</td>\n",
       "      <td>0.04250</td>\n",
       "      <td>0.00980</td>\n",
       "      <td>0.05230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>2.32280</td>\n",
       "      <td>2.28810</td>\n",
       "      <td>2.28720</td>\n",
       "      <td>0.03470</td>\n",
       "      <td>0.00090</td>\n",
       "      <td>0.03560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>2.25525</td>\n",
       "      <td>2.22425</td>\n",
       "      <td>2.21000</td>\n",
       "      <td>0.03100</td>\n",
       "      <td>0.01425</td>\n",
       "      <td>0.04525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>2.31700</td>\n",
       "      <td>2.28505</td>\n",
       "      <td>2.27480</td>\n",
       "      <td>0.03195</td>\n",
       "      <td>0.01025</td>\n",
       "      <td>0.04220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>2.34070</td>\n",
       "      <td>2.30545</td>\n",
       "      <td>2.28155</td>\n",
       "      <td>0.03525</td>\n",
       "      <td>0.02390</td>\n",
       "      <td>0.05915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>2.56420</td>\n",
       "      <td>2.52575</td>\n",
       "      <td>2.50820</td>\n",
       "      <td>0.03845</td>\n",
       "      <td>0.01755</td>\n",
       "      <td>0.05600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>2.00595</td>\n",
       "      <td>1.98365</td>\n",
       "      <td>1.97280</td>\n",
       "      <td>0.02230</td>\n",
       "      <td>0.01085</td>\n",
       "      <td>0.03315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    song_id  avg_weight0  avg_weight1  avg_weight2  avg_water_loss1  \\\n",
       "0         0      2.15330      2.10955      2.09665          0.04375   \n",
       "1         1      2.12135      2.08655      2.07245          0.03480   \n",
       "2         2      1.91580      1.86810      1.86085          0.04770   \n",
       "3         3      1.82995      1.77550      1.76390          0.05445   \n",
       "4         4      2.44275      2.37980      2.37050          0.06295   \n",
       "5         5      2.21200      2.16675      2.15565          0.04525   \n",
       "6         6      2.43565      2.39155      2.38010          0.04410   \n",
       "7         7      2.48585      2.44075      2.42585          0.04510   \n",
       "8         8      3.27330      3.21245      3.19370          0.06085   \n",
       "9         9      3.24150      3.19515      3.17785          0.04635   \n",
       "10       10      2.01935      1.98205      1.97005          0.03730   \n",
       "11       11      2.27030      2.21955      2.20590          0.05075   \n",
       "12       12      2.31520      2.27295      2.26300          0.04225   \n",
       "13       13      2.20380      2.16010      2.15110          0.04370   \n",
       "14       14      2.20545      2.16840      2.15680          0.03705   \n",
       "15       15      2.37940      2.34090      2.33110          0.03850   \n",
       "16       16      2.09155      2.05500      2.04540          0.03655   \n",
       "17       17      2.21270      2.17320      2.16025          0.03950   \n",
       "18       18      2.47340      2.41635      2.39850          0.05705   \n",
       "19       19      2.19585      2.15805      2.14555          0.03780   \n",
       "20       20      2.05615      2.01970      2.00675          0.03645   \n",
       "21       21      2.29355      2.23680      2.22325          0.05675   \n",
       "22       22      2.33575      2.29475      2.27560          0.04100   \n",
       "23       23      2.30820      2.26680      2.25005          0.04140   \n",
       "24       24      2.53975      2.49390      2.47980          0.04585   \n",
       "25       25      2.06395      2.02670      2.01365          0.03725   \n",
       "26       26      2.26795      2.22315      2.21160          0.04480   \n",
       "27       27      1.96460      1.91845      1.90915          0.04615   \n",
       "28       28      2.38225      2.33015      2.32405          0.05210   \n",
       "29       29      2.34140      2.29150      2.27720          0.04990   \n",
       "30       30      2.65305      2.59760      2.58695          0.05545   \n",
       "31       31      2.10255      2.05030      2.04230          0.05225   \n",
       "32       32      2.09910      2.06165      2.05065          0.03745   \n",
       "33       33      2.40690      2.36495      2.34725          0.04195   \n",
       "34       34      1.98990      1.95965      1.94915          0.03025   \n",
       "35       35      2.24795      2.20805      2.19890          0.03990   \n",
       "36       36      2.01670      1.98560      1.97275          0.03110   \n",
       "37       37      2.27710      2.24255      2.23000          0.03455   \n",
       "38       38      2.41835      2.37710      2.36575          0.04125   \n",
       "39       39      2.11270      2.07560      2.06605          0.03710   \n",
       "40       40      2.20500      2.16250      2.15270          0.04250   \n",
       "41       41      2.32280      2.28810      2.28720          0.03470   \n",
       "42       42      2.25525      2.22425      2.21000          0.03100   \n",
       "43       43      2.31700      2.28505      2.27480          0.03195   \n",
       "44       44      2.34070      2.30545      2.28155          0.03525   \n",
       "45       45      2.56420      2.52575      2.50820          0.03845   \n",
       "46       46      2.00595      1.98365      1.97280          0.02230   \n",
       "\n",
       "    avg_water_loss2  avg_total_water_loss  \n",
       "0           0.01290               0.05665  \n",
       "1           0.01410               0.04890  \n",
       "2           0.00725               0.05495  \n",
       "3           0.01160               0.06605  \n",
       "4           0.00930               0.07225  \n",
       "5           0.01110               0.05635  \n",
       "6           0.01145               0.05555  \n",
       "7           0.01490               0.06000  \n",
       "8           0.01875               0.07960  \n",
       "9           0.01730               0.06365  \n",
       "10          0.01200               0.04930  \n",
       "11          0.01365               0.06440  \n",
       "12          0.00995               0.05220  \n",
       "13          0.00900               0.05270  \n",
       "14          0.01160               0.04865  \n",
       "15          0.00980               0.04830  \n",
       "16          0.00960               0.04615  \n",
       "17          0.01295               0.05245  \n",
       "18          0.01785               0.07490  \n",
       "19          0.01250               0.05030  \n",
       "20          0.01295               0.04940  \n",
       "21          0.01355               0.07030  \n",
       "22          0.01915               0.06015  \n",
       "23          0.01675               0.05815  \n",
       "24          0.01410               0.05995  \n",
       "25          0.01305               0.05030  \n",
       "26          0.01155               0.05635  \n",
       "27          0.00930               0.05545  \n",
       "28          0.00610               0.05820  \n",
       "29          0.01430               0.06420  \n",
       "30          0.01065               0.06610  \n",
       "31          0.00800               0.06025  \n",
       "32          0.01100               0.04845  \n",
       "33          0.01770               0.05965  \n",
       "34          0.01050               0.04075  \n",
       "35          0.00915               0.04905  \n",
       "36          0.01285               0.04395  \n",
       "37          0.01255               0.04710  \n",
       "38          0.01135               0.05260  \n",
       "39          0.00955               0.04665  \n",
       "40          0.00980               0.05230  \n",
       "41          0.00090               0.03560  \n",
       "42          0.01425               0.04525  \n",
       "43          0.01025               0.04220  \n",
       "44          0.02390               0.05915  \n",
       "45          0.01755               0.05600  \n",
       "46          0.01085               0.03315  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg = pd.read_csv('/Users/rgu/Desktop/UROPs/UROP4/repo/dataframes/avg_diego_47.csv')\n",
    "display(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(paths, sr = 22050, fixed_length = 1323000):\n",
    "    '''\n",
    "    loads audios from audio path for training/testing\n",
    "    '''\n",
    "    audio_data = []\n",
    "    for filepath in paths:\n",
    "        y, sr = librosa.load(filepath, sr=sr)\n",
    "        if len(y) < fixed_length: #ensures audios are same length\n",
    "            y = np.pad(y, (0, fixed_length - len(y)), 'constant')\n",
    "        else:\n",
    "            y = y[:fixed_length]\n",
    "\n",
    "        audio_data.append(y)\n",
    "\n",
    "    #print(audio_data)\n",
    "    return np.array(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_data, targets):\n",
    "        self.audio_data = audio_data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio = self.audio_data[idx]\n",
    "        target = self.targets[idx]\n",
    "        return torch.tensor(audio, dtype=torch.float32), torch.tensor(target, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load audio to an array\n",
    "path = '/Users/rgu/Desktop/UROPs/UROP4/diego_100_audios'\n",
    "audio_files = [os.path.join(path, filename) for filename in os.listdir(path) if filename.lower().endswith('.mp3')]\n",
    "audio_data = load_audio_files(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = audio_data[:47] #REMOVE THIS AFTER EXPERIMENTS, we only have 47 datapoints atm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_loss = np.array(avg['avg_water_loss1'])  # TRAINING ON WATER LOSS OF MINUTE 1\n",
    "\n",
    "#create dataset and data loader\n",
    "train_size = int(len(audio_data)* 0.8)\n",
    "test_size = len(audio_data) - train_size\n",
    "\n",
    "dataset = AudioDataset(audio_data, water_loss) #because we only have 47 water losses\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_set, batch_size=2, shuffle=True) \n",
    "test_loader = DataLoader(test_set, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaterLossCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Define the CNN model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(WaterLossCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * (audio_data.shape[1] // 4), 64) #accomodate for flattening\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * (audio_data.shape[1] // 4)) #flattens the data\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaterLossCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() #loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) #weight optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs = 100):\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.unsqueeze(1)  #add channel dimension: (batchsize, seq_len) -> (batchsize, 1, seq_len)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #implement validation later when you have more data\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 326.1878356933594\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 11\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#implement validation later when you have more data\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/urop4/lib/python3.9/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/urop4/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/urop4/lib/python3.9/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/urop4/lib/python3.9/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/urop4/lib/python3.9/site-packages/torch/optim/adam.py:439\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    437\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_targets.extend(targets.numpy())\n",
    "            all_predictions.extend(outputs.numpy().flatten())\n",
    "\n",
    "        all_targets = np.array(all_targets)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        mae = mean_absolute_error(all_targets, all_predictions)\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return f\"Average Loss: {avg_loss}, Mean Absolute Error:{mae}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_model(trained_model, test_loader, criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict new audio file's water loss\n",
    "new_audio_file = 'new_file.wav'\n",
    "new_audio_data = load_audio_files([new_audio_file])\n",
    "new_audio_tensor = torch.tensor(new_audio_data, dtype=torch.float32).unsqueeze(1)  # Add batch and channel dimensions\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_water_loss = model(new_audio_tensor)\n",
    "    print('Predicted Water Loss:', predicted_water_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
